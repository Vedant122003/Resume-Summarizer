{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b29e389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vedan\\Desktop\\GenAIProject\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from pypdf import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c112cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        extracted = page.extract_text()\n",
    "        if extracted:\n",
    "            text += extracted + \"\\n\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d0fb5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=300, overlap=50):\n",
    "\n",
    "    if overlap >= chunk_size:\n",
    "        raise ValueError(\"overlap must be smaller than chunk_size\")\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "\n",
    "    while start < text_length:\n",
    "\n",
    "        end = min(start + chunk_size, text_length)\n",
    "\n",
    "        chunk = text[start:end]\n",
    "\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        start += chunk_size - overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3837e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 728.36it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def create_embeddings(chunks):\n",
    "    embeddings = embedding_model.encode(chunks)\n",
    "    return np.array(embeddings).astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98623e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(embeddings):\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39c64dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 201/201 [00:00<00:00, 531.24it/s, Materializing param=model.norm.weight]                              \n"
     ]
    }
   ],
   "source": [
    "llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    device=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5c8a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(query, index, chunks):\n",
    "\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    query_embedding = np.array(query_embedding).astype(\"float32\")\n",
    "    k = 3\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    retrieved_chunks = [chunks[i] for i in indices[0]]\n",
    "    context = \"\\n\".join(retrieved_chunks)\n",
    "    prompt = f\"\"\"\n",
    "<|system|>\n",
    "You are a helpful assistant. Answer only from the context below.\n",
    "If answer is not present, say \"Not found in resume\".\n",
    "\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "    response = llm(\n",
    "        prompt,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.3,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    answer = response[0][\"generated_text\"]\n",
    "    answer = answer.split(\"<|assistant|>\")[-1].strip()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c0ce989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 10\n"
     ]
    }
   ],
   "source": [
    "resume_text = load_pdf(r\"C:\\Users\\vedan\\Desktop\\GenAIProject\\resources\\resume.pdf\")\n",
    "chunks = chunk_text(resume_text)\n",
    "\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "embeddings = create_embeddings(chunks)\n",
    "index = create_faiss_index(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17171829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing `generation_config` together with generation-related arguments=({'max_new_tokens', 'do_sample', 'temperature'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=200) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:\n",
      " What are Vedant's technical skills?\n",
      "\n",
      "Answer:\n",
      " Vedant Sinha's technical skills include Java, Python, JavaScript, TypeScript, Spring Boot, Angular, Selenium, and TestNG.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are Vedant's technical skills?\"\n",
    "answer = ask_question(query, index, chunks)\n",
    "print(\"\\nQuestion:\\n\", query)\n",
    "print(\"\\nAnswer:\\n\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
